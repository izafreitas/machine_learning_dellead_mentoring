{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52277551",
   "metadata": {},
   "source": [
    "# Spacy for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155c879",
   "metadata": {},
   "source": [
    "In this project, sentiment analysis was done using natural language processing on the online reviews prevalant for various items on amazon,yelp and imdb which were lablelled. Using the spacy package of python to preprocess the data before, each individual review has been tokenized, lemmatized, filtered for stop words and vectorized inorder to prepare the data viable for the machine learning model. A pipeline was created which vectorized the preprocessed data using count vectorization or tfidf vectorizer, which is then split into training and testing datasets and were then used to train the machine learning model (support vector machine) and evaluate.\n",
    "\n",
    "https://mahadev001.github.io/Mahadev-Upadhyayula/Sentiment%20Analysis%20via%20NLP/Sentiment%20Analysis%20using%20NLP%20with%20Spacy%20and%20%20SVM.html\n",
    "\n",
    "\n",
    "The data set contains about 1000 online reviews each for various items on Amazon, Yelp and IMDB and of these reviews about 500 were labelled positive and 500 were labelled negative reviews. For each company, the data was given the text format which are needed to be added to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "518b7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ec097",
   "metadata": {},
   "source": [
    "# Importanto e explorando dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed207f20",
   "metadata": {},
   "source": [
    "Os dados de cada uma das empresas são armazenados separadamente em arquivos txt. Cada um desses arquivos foi importado separadamente e associado a campos-chave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb682f19",
   "metadata": {},
   "source": [
    "## Importanto e reunindo dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e323b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yelp = pd.read_table('yelp_labelled.txt')\n",
    "data_amazon = pd.read_table('amazon_cells_labelled.txt')\n",
    "data_imdb = pd.read_table('imdb_labelled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8b3ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_col=[data_amazon, data_imdb, data_yelp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "952d1870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    So there is no way for me to plug it in here in the US unless I go by a converter.  \\\n",
      "0                          Good case, Excellent value.                                   \n",
      "1                               Great for the jawbone.                                   \n",
      "2    Tied to charger for conversations lasting more...                                   \n",
      "3                                    The mic is great.                                   \n",
      "4    I have to jiggle the plug to get it to line up...                                   \n",
      "..                                                 ...                                   \n",
      "994  The screen does get smudged easily because it ...                                   \n",
      "995  What a piece of junk.. I lose more calls on th...                                   \n",
      "996                       Item Does Not Match Picture.                                   \n",
      "997  The only thing that disappoint me is the infra...                                   \n",
      "998  You can not answer calls with the unit, never ...                                   \n",
      "\n",
      "     0  \n",
      "0    1  \n",
      "1    1  \n",
      "2    0  \n",
      "3    1  \n",
      "4    0  \n",
      "..  ..  \n",
      "994  0  \n",
      "995  0  \n",
      "996  0  \n",
      "997  0  \n",
      "998  0  \n",
      "\n",
      "[999 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-db5c6eb0dc9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_amazon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "print(data_amazon).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5cb70b",
   "metadata": {},
   "source": [
    "### Adicionando cabeçalhos nas colunas de cada conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2a00678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Review', 'Label'], dtype='object')\n",
      "Index(['A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  ', '0'], dtype='object')\n",
      "Index(['Wow... Loved this place.', '1'], dtype='object')\n",
      "Index(['Review', 'Label'], dtype='object')\n",
      "Index(['Review', 'Label'], dtype='object')\n",
      "Index(['Wow... Loved this place.', '1'], dtype='object')\n",
      "Index(['Review', 'Label'], dtype='object')\n",
      "Index(['Review', 'Label'], dtype='object')\n",
      "Index(['Review', 'Label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for colname in combined_col:\n",
    "    colname.columns = [\"Review\", \"Label\"]\n",
    "    for colname in combined_col:\n",
    "        print(colname.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dbb47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para saber a qual empresa pertence o conjunto de dados foi adicionado um a coluna \"Company\" como chave\n",
    "company = [\"Amazon\", \"imdb\", \"yelp\"]\n",
    "comb_data = pd.concat(combined_col, keys = company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ef3a793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2745, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                        Review  Label\n",
       "Amazon 0                          Good case, Excellent value.      1\n",
       "       1                               Great for the jawbone.      1\n",
       "       2    Tied to charger for conversations lasting more...      0\n",
       "       3                                    The mic is great.      1\n",
       "       4    I have to jiggle the plug to get it to line up...      0\n",
       "...                                                       ...    ...\n",
       "yelp   994  I think food should have flavor and texture an...      0\n",
       "       995                           Appetite instantly gone.      0\n",
       "       996  Overall I was not impressed and would not go b...      0\n",
       "       997  The whole experience was underwhelming, and I ...      0\n",
       "       998  Then, as if I hadn't wasted enough of my life ...      0\n",
       "\n",
       "[2745 rows x 2 columns]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explorando a estrutura do novo dataframe\n",
    "print(comb_data.shape)\n",
    "comb_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40dc3c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Review', 'Label'], dtype='object')\n",
      "Review    0\n",
      "Label     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "comb_data.to_csv(\"Sentiment_Analysis_Dataset\")\n",
    "print(comb_data.columns)\n",
    "print(comb_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc4a18",
   "metadata": {},
   "source": [
    "# Pré-processamento dos dados usando Spacy e treinamento do modelo de aprendizado de máquina usando sklearn ¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b81b58",
   "metadata": {},
   "source": [
    "Neste estágio, o pacote Spacy do python é usado para lematizar e remover palavras de parada do conjunto de dados obtido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44053937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['further', 'then', 'though', 'five', 'will', 'your', 'latter', 'am', 'throughout', 'first', 'last', 'ours', 'becoming', 'everything', 'them', 'seem', 'him', 'side', 'whose', 'just', 'it', 'sometimes', 'anywhere', 'seeming', 're', 'ca', 'often', 'once', 'full', 'show', 'two', 'say', 'its', 'all', 'whereas', 'can', '’m', 'hundred', 'here', 'ourselves', 'thereupon', 'so', 'per', 'we', 'regarding', 'four', 'after', 'against', 'around', 'everyone', '‘d', 'take', 'bottom', 'anyone', 'front', 'nothing', 'since', 'thereby', 'under', 'using', 'onto', 'forty', 'put', 'indeed', 'something', 'serious', 'alone', 'from', 'also', 'through', 'somewhere', 'made', 'who', 'being', 'yours', 'each', 'if', 'why', 'thence', 'in', \"'ve\", 'please', 'enough', 'mostly', 'within', 'therein', 'and', 'themselves', 'were', 'every', 'that', 'while', '‘re', 'where', 'into', 'empty', 'those', 'three', 'whereupon', 'whither', 'less', 'sixty', \"n't\", 'as', 'ever', 'almost', 'us', 'whereby', 'herein', 'yet', 'to', 'could', 'six', 'everywhere', 'which', 'would', 'across', 'any', 'eleven', 'itself', 'have', 'yourselves', 'whereafter', 'she', 'my', 'same', 'anything', 'fifty', 'name', 'make', 'mine', 'out', 'during', 'call', 'over', 'whenever', 'next', 'other', 'nevertheless', 'an', 'seemed', 'still', 'anyhow', 'thus', 'he', 'with', 'thereafter', '’ve', 'many', 'on', 'move', 'down', 'together', 'twenty', 'hers', 'do', '‘ve', 'third', 'becomes', 'only', 'behind', 'myself', 'was', 'without', 'some', 'done', 'another', 'perhaps', 'his', 'part', \"'re\", 'such', 'always', 'least', 'very', 'well', 'used', \"'d\", 'above', 'became', 'what', 'nine', 'amongst', 'various', 'again', 'for', 'how', 'someone', 'sometime', 'noone', 'whoever', 'both', 'doing', 'too', 'you', 'this', 'although', 'never', 'me', 'twelve', 'until', '‘m', 'does', 'beside', 'hereafter', \"'m\", '’re', 'give', 'nobody', 'hence', 'much', 'a', 'her', 'not', 'has', 'up', 'is', 'none', 'n’t', '‘ll', 'when', 'already', 'formerly', 'get', 'the', 'below', 'hereupon', 'anyway', 'else', 'no', 'nor', 'become', 'there', 'whether', 'amount', 'beyond', 'moreover', 'whom', 'whatever', 'wherein', 'of', 'others', 'towards', 'back', 'otherwise', 'therefore', 'herself', 'by', 'except', 'namely', 'fifteen', '’s', 'because', 'really', 'had', 'these', '‘s', 'himself', 'might', 'see', 'their', 'n‘t', 'about', 'our', 'now', 'toward', 'few', 'unless', 'i', 'nowhere', 'yourself', 'meanwhile', 'more', 'must', 'whole', 'several', 'keep', 'somehow', 'either', 'thru', 'be', 'former', 'even', 'own', 'latterly', 'via', '’d', \"'s\", 'most', 'or', 'along', 'whence', 'they', 'neither', 'among', 'rather', 'off', 'did', 'elsewhere', 'between', 'than', 'at', 'eight', 'upon', 'ten', \"'ll\", 'afterwards', 'should', 'hereby', '’ll', 'quite', 'however', 'been', 'before', 'are', 'may', 'cannot', 'but', 'go', 'besides', 'one', 'top', 'beforehand', 'wherever', 'seems', 'due']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#nlp = en_core_web_sm.load()\n",
    "#nlp = spacy.load('en')\n",
    "# Para construir uma lista de stop words para filtragem\n",
    "stopwords = list(STOP_WORDS)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44fa8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "punctuations = string.punctuation\n",
    "# Criando um Parser Spacy\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "381ac421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer (sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lema_.lower().strip() if word.lema_ != \"-PRON-\" else word.lower_ for word in mytokens]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5bf125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacotes de ML \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29f5139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizando o transformador usando SpaCy\n",
    "class preditors (TransformerMixin):\n",
    "    def transform (self, X, **transform_params):\n",
    "        return[clean_text (text) for text in X]\n",
    "    def fit (self, X, y, **fit_params):\n",
    "        return self\n",
    "# Função básica para limpar o texto\n",
    "def clean_text (text):\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9dc51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorization\n",
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8263cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando o TF-IDF\n",
    "tfvectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19209927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo o dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8fc72c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas e rótulos\n",
    "X = comb_data['Review']\n",
    "ylabels = comb_data['Label']\n",
    "\n",
    "#Dividindo em teste e treino\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15013d9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-362533983674>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Criando a pipeline para limpar, tokenizar, vetorizar e classificar usando o \"CountVectorizer\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpipe_countvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cleaner\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'vectorizer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'classfier'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Dando o fit nos dados - ajustando\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpipe_countvect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictors' is not defined"
     ]
    }
   ],
   "source": [
    "# Criando a pipeline para limpar, tokenizar, vetorizar e classificar usando o \"CountVectorizer\"\n",
    "pipe_countvect = Pipeline([(\"cleaner\", predictors()), ('vectorizer', vectorizer), ('classfier', classifier)])\n",
    "\n",
    "# Dando o fit nos dados - ajustando\n",
    "pipe_countvect,fit(X_train, y_train)\n",
    "# Fazendo as predições com o conjunto de dados de teste\n",
    "sample_prediction = pipe_countvect.predict(X_test)\n",
    "\n",
    "# Resultado das predições\n",
    "# 1 = Positive review\n",
    "# 0 = Negative review\n",
    "for (sample, pred) in zip (X_test, sample_prediction):\n",
    "    print(sample, 'Prediction=>', pred)\n",
    "    \n",
    "    \n",
    "# Accuracy\n",
    "print(\"Accuracy: \", pipe_countvect.score(X_test, y_test))\n",
    "print(\"Accuracy: \", pipe_countvect.score(X_test, sample_prediction))\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy: \", pipe_countvect.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ffec981a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-1af8610f4141>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Outra review aleatória\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"This was a great movie\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "# Outra review aleatória\n",
    "pipe.predict([\"This was a great movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "887a233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\" I do enjoy my job\", \"What a poor product!, I will have to get a new one\", \"I feel amazing!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9ea73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edcf847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
